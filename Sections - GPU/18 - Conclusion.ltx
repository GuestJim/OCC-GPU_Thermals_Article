\section*{Conclusion}

This has certainly been an interesting and enjoyable experience for me as I have re-worked the scripts, added some new features, and made some discoveries concerning the behaviors of the different GPUs in my collection. It definitely makes me wish I had more in my collection to experiment with, but this has not been a quick article to write, so maybe this is best.

The greatest surprise to me was the power consumption behavior as none of the graphics cards actually ran at their stated TDP as I had expected. Until the Turing GPUs from NVIDIA, they would peak but then fall to levels away from the specification. While the RTX 2060 and RTX 2080 did manage to keep power draw at or near the TDP, they also both saw power consumption become rather unstable as time progressed. While I can recognize the electronics involved are likely completely fine with noisy bouncing up and down, I cannot help but be a little concerned when looking at the relevant Course graphs.

The RX Vega 64 kept its power consumption very stable, but at 245 W, a full 50 W below its stated TDP. Actually that makes me think the specification might be wrong, especially as checking some old data, from my original reference RX Vega 64 showed it pulling only 220 W at most. Why the specification would so tremendously surpass what is actually being consumed, I do not know. Even if we account for the other electronics on the card not included in the measurement, such as the fans and any LEDs, that would not account for the 50 W, or more in the case of the reference card. Perhaps it was to act as a buffer for overclocking, or an intentional overshoot so cooler designs would be more capable than necessary. These guesses do not seem completely unreasonable to me, but I really do not know. Now I want to take back what I said earlier because having a newer AMD GPU to test would have been great, to see if the RDNA designs behave differently.

Another reason it would have been interesting to have a new AMD GPU is to see if the boosting behavior has changed, because that is definitely something that differed between the RX Vega 64 and all of the NVIDIA GPUs. All of the NVIDIA GPUs demonstrated a boost behavior like I would expect from a CPU, where it would start at a high frequency and slow over a relatively short period of time. For a CPU where many of its tasks can be expected to finish in minutes or seconds, this makes sense, but a gaming graphics card can be expected to be engaged for easily an hour or more at a time. Perhaps the 3DMark tests are not the best to use for this, which would be a fair point to make, but we can still clearly see behavior not unlike a boost-budget approach, especially because the behavior so changed for the pulsing tests. If you compare the frequency histograms for the RX Vega 64 you will see the placement of the distributions between looping and pulsing are very similar, with the difference mainly being the amount of data, while many of the NVIDIA GPUs saw the distributions shift or new peaks emerge. This could also be explained by sensitivity to the temperature of the GPU, but it remains odd to me then that the frequency would start falling before the temperature reached the expected throttling thresholds. Of course some of the NVIDIA GPUs, mainly the newer ones, were able to stay above their boost clocks as though they were actually base clocks, so the performance is clearly there, but it is still not quite the behavior I would expect.

Wrapping this up, I do wonder when I may next use these scripts as there is little more I can do with them, without adding to my collection of graphics cards. Considering the current state of the market, it might be a while before that happens, but at least these will be ready.
