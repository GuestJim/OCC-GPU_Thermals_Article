\section{Time Series Rough Explanation}
Since the original article when I used these scripts, I have made a number of changes to them, including what should be some interesting additions. I want to cover one of those additions here, so I do not need to repeat an explanation of it when getting to the graphs. Before really getting into though, I should explain that I have a very limited understanding of this topic, so my explanation may be rough and somewhat inaccurate, but I will do the best I can.

This topic is time series, a pretty useful tool in statistics, depending on the data you are working with. Imagine owning a restaurant and you want to know how your business is doing, so you look at the number of customers you have had over the past so many weeks. Plotting it out you see each week had a certain pattern to it, such as more customers coming in on the weekends than the weekdays. This poses a problem because the rise and fall of this pattern may obscure the trend you are interested in; if your business is going up, down, or remaining the same. This is the kind of situation where time series analysis makes sense, because it is able to recognize that weekly pattern and remove it, so the trend can be seen.

When looping a synthetic benchmark, like those in 3DMark, the scenario is much the same because over the length of a single run, the frame time will increase and decrease following some regular pattern. What we are interested in is a larger pattern across multiple runs, and that is why I decided to figure out how to use time series. I am applying it to frame time as well as GPU frequency, power, and temperature, as those too may have some pattern to them resulting from the workload.

What the R functions for time series will do is take the original data and decompose it to three separate components: seasonal; trend; and random. The seasonal component is the regular pattern, such as the weekly customers for the restaurant or the frame times for a single run of "Fire Strike" or "Time Spy." The trend component, as its name suggests, is what we are interested in as that is the way the seasonal component is adjusted up and down. The random component then is what remains, like noise in the system that does not fit into either of the other components.

It is important to know there are two types of time series, which is based on how these three components are related. For an additive time series, the sum of these three components will give you the original data. For a multiplicative time series, the product of the components gets you the original. I believe this data represents an additive series because a characteristic of one is that the seasonal pattern remains fairly consistent as far as its peaks and troughs are concerned, while a multiplicative series will see them become more exaggerated over time. There is no hard and fast rule for this though, so it does also come down to one's judgement.

Something very important about the seasonal and random components is they are balanced around 0. This means their own overall trend is nothing, which I have seen when plotting them and adding a smooth line layer to the graph, but at any arbitrary time they may have non-zero values. It is only the trend component that is unbalanced and so captures if the overall data moved up and down. The reason this is important to know is because the specific values on the trend graphs are not necessarily correct representations of the original data. There are some where, by coincidence, the real values and trend values align, but they should not be trusted in general.

Despite this, for the frame time and frequency graphs I have added labels to mark the minimum and maximum to the smooth lines. While I do not believe these specific values can be trusted, especially as they are from the smoothing model and not even the trend data itself, I do still feel the range for this smooth line can be interesting to see. The power and temperature graphs I do not feel need these labels because they tend not to be as busy as the other two. You will see what I mean when we get into the results.

Something that is necessary when processing a time series is to know how long the seasonal pattern is. For the measurements from GPU-z this is relatively easy because it samples every second, but PresentMon data has a variable period between measurements. Ultimately I decided on passing the PresentMon time series the length of the test period in seconds, so, in theory, it is looking at each second, while for the GPU-z data it is being given the length of each run, approximately. For the looping tests it will just use the length of each run, which is 30 seconds for "Fire Strike" and 60 seconds for "Time Spy." I decided to do some experimenting with the pulsing tests though, because there is loading time that impacts the length of the seasonal pattern. For "Fire Strike" I found applying an offset of 9 seconds to the test length was successful at putting more of the variability into the seasonal component.

For "Time Spy" I found an offset of -15 achieved a similar result, but I will be honest that I have no idea why a negative value would work. Still, by working from the temperature data I was able to see the variability as the GPU heated and cooled with each run disappear from the trend data and instead be present on the seasonal. This makes the temperature trend smoother and also pulled out more information from the power and frequency trends, so they are not just a triangle signal going up and down, and these were my goals when experimenting with these offsets.
